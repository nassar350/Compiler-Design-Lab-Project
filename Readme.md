# C Compiler Frontend: Lexical Analyzer (Scanner) + Syntax Analyzer (Parser)

A comprehensive compiler frontend implementation in Python that performs both lexical analysis and syntax analysis for C-like source code. The project consists of a complete scanner (tokenizer) and a recursive-descent parser that validates syntax according to a formal grammar specification.

---

## ğŸ—‚ Project Structure

```
Design Project (Scanner + Parser)/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ lexer/
â”‚   â”‚   â”œâ”€â”€ token.py            # Token class definition
â”‚   â”‚   â”œâ”€â”€ token_types.py      # Token type enum
â”‚   â”‚   â”œâ”€â”€ scanner.py          # Helper methods for token classification
â”‚   â”‚   â””â”€â”€ tokenizer.py        # Main tokenizer logic (lexical analysis)
â”‚   â”‚
â”‚   â”œâ”€â”€ parser/
â”‚   â”‚   â”œâ”€â”€ Grammar.txt         # Formal BNF grammar specification
â”‚   â”‚   â””â”€â”€ parser.py           # Recursive-descent parser implementation
â”‚   â”‚
â”‚   â”œâ”€â”€ IO/
â”‚   â”‚   â”œâ”€â”€ file_reader.py      # Reads source code from file
â”‚   â”‚   â””â”€â”€ file_writer.py      # Writes tokens to output file
â”‚   â”‚
â”‚   â”œâ”€â”€ main.py                 # Entry point (CLI) - integrates scanner + parser
â”‚   â”œâ”€â”€ SourceCode.c            # Sample C source code for testing
â”‚   â””â”€â”€ result.json             # Token output file
â”‚
â””â”€â”€ Readme.md
```
---

## âš¡ Features

### Lexical Analyzer (Scanner)
- **Comprehensive Token Recognition:**
  - **Keywords** (`int`, `float`, `void`, `if`, `else`, `while`, `for`, `return`)
  - **Identifiers** (variable/function names)
  - **Operators** (`+`, `-`, `*`, `/`, `=`, `==`, `!=`, `<`, `<=`, `>`, `>=`, `&&`, `||`)
  - **Numeric Constants** (integers and floats)
  - **Character Constants** (`'a'`, `'1'`)
  - **Special Characters** (`(){}[];,#.`)
  - **Comments** (`// single-line`, `/* multi-line */`)
  - **Whitespace & Newlines**
- Token statistics and summary reporting
- JSON output for token sequences

### Syntax Analyzer (Parser)
- **Recursive-Descent Parser** implementation
- **Complete Grammar Support** for C-like syntax:
  - Program structure with declarations
  - Variable declarations (single and multiple)
  - Function declarations with parameters
  - Compound statements with local variables
  - Control flow statements (`if`/`else`, `while`, `for`, `return`)
  - Expression parsing with proper precedence:
    - Assignment expressions
    - Logical operators (`||`, `&&`)
    - Relational operators (`<`, `<=`, `>`, `>=`, `==`, `!=`)
    - Arithmetic operators (`+`, `-`, `*`, `/`)
  - Function calls with arguments
  - Parenthesized expressions
- **Smart Trivia Handling**: Automatically skips comments, whitespace, and newlines during parsing
- **Comprehensive Error Reporting**: Clear syntax error messages with context
- **Lookahead Support**: Multi-token lookahead for disambiguation

---

## ğŸ’» Requirements

- Python 3.7+
- No external dependencies (uses built-in modules like `re` and `argparse`)

---

## ğŸš€ Usage

### Command-Line Interface

```bash
python main.py input_file.c -o tokens_output.json
```
- `input_file.c` â†’ Source code file to tokenize and parse
- `-o tokens_output.json` â†’ Optional output file to save tokens

### Execution Flow

1. **Lexical Analysis**: Source code is tokenized into a stream of tokens
2. **Token Statistics**: Display token counts by type
3. **Syntax Analysis**: Parser validates the token stream against grammar rules
4. **Result**: Outputs "Syntax: OK" if valid, or reports syntax errors with details
5. **Output**: Optionally writes tokens to specified file

### Example

```bash
python .\main.py .\SourceCode.c
```

---

## ğŸ“Š Example Output

### Sample Input (SourceCode.c)
```c
int main() {
    int x, y;
    if (x == 42) {
        x = x - 3;
    } else {
        y = 3.1;
    }
    return 0;
}
```

### Console Output
```
Total Tokens: 45

Token Type Counts:
  KEYWORD         : 5
  IDENTIFIER      : 6
  OPERATOR        : 5
  NUMERIC_CONSTANT: 3
  SPECIAL_CHARACTER: 20
  COMMENT         : 3
  WHITESPACE      : 2
  NEWLINE         : 1

Tokens:
[Token (int,TokenType.KEYWORD), Token (main,TokenType.IDENTIFIER), ...]

==============================

Syntax: OK
âœ… Tokens written to result.json
```
---

## ğŸ”§ How It Works

### Phase 1: Lexical Analysis (Scanner)

1. `main.py` reads the input source file via `file_reader.py`
2. `Tokenizer` uses regex patterns and helper methods in `Scanner` to classify each lexeme
3. Tokens are stored as instances of the `Token` class with an associated `TokenType`
4. A summary of token counts by type is displayed
5. Token stream is passed to the parser

### Phase 2: Syntax Analysis (Parser)

1. `Parser` receives the token stream from the tokenizer
2. Implements recursive-descent parsing based on formal grammar (`Grammar.txt`)
3. Each non-terminal in the grammar has a corresponding `parse_*` method
4. Parser automatically skips trivia (comments, whitespace, newlines) during analysis
5. Uses lookahead to disambiguate grammar productions (e.g., variable vs function declaration)
6. Validates proper nesting of scopes and statement structures
7. Reports syntax errors with descriptive messages if validation fails
8. Confirms successful parse with "Syntax: OK" message

### Key Parser Features

- **Trivia Handling**: `_skip_trivia()` automatically ignores whitespace/comments between tokens
- **Lookahead**: `peek(n)` allows multi-token lookahead for parsing decisions
- **Error Recovery**: Clear error messages indicating expected vs actual tokens
- **Grammar Coverage**: Full implementation of C-like syntax including expressions, statements, and declarations

---

## ğŸ“Š Token Types

| Token Type            | Example                     | Description                               |
|-----------------------|-----------------------------|-------------------------------------------|
| KEYWORD               | `int`, `float`, `if`, `while` | Reserved words in the language          |
| IDENTIFIER            | `x`, `main`, `myVar`        | Variable or function names                |
| OPERATOR              | `+`, `-`, `==`, `&&`        | Arithmetic, relational, logical operators |
| NUMERIC_CONSTANT      | `42`, `3.14`                | Integer or floating-point numbers         |
| CHARACTER_CONSTANT    | `'a'`, `'1'`                | Single character literals                 |
| SPECIAL_CHARACTER     | `(`, `)`, `{`, `}`, `;`     | Punctuation and delimiters               |
| COMMENT               | `// ...`, `/* ... */`       | Single-line or multi-line comments       |
| WHITESPACE            | ` `, `\t`                   | Spaces and tabs                          |
| NEWLINE               | `\n`                        | Line breaks                              |

---

## ğŸ“ Grammar Specification

The parser implements a complete C-like grammar defined in `Grammar.txt`. Key grammar rules include:

### Program Structure
```
Program        â†’ DeclList EOF
DeclList       â†’ Decl DeclList | Îµ
Decl           â†’ VarDecl | FunDecl
```

### Declarations
```
VarDecl        â†’ TypeSpec ID VarDeclTail ;
FunDecl        â†’ TypeSpec ID ( ParamList ) CompoundStmt
TypeSpec       â†’ int | float | void
```

### Statements
```
Stmt           â†’ ExprStmt | CompoundStmt | IfStmt | WhileStmt | ForStmt | ReturnStmt
IfStmt         â†’ if ( Expr ) Stmt ElsePart
WhileStmt      â†’ while ( Expr ) Stmt
ForStmt        â†’ for ( ForInitExpr ; ForCondExpr ; ForIterExpr ) Stmt
ReturnStmt     â†’ return ExprOpt ;
```

### Expressions (with precedence)
```
Expr           â†’ AssignExpr
AssignExpr     â†’ ID = AssignExpr | OrExpr
OrExpr         â†’ AndExpr OrExprTail
AndExpr        â†’ RelExpr AndExprTail
RelExpr        â†’ AddExpr RelOpTail
AddExpr        â†’ Term AddExprTail
Term           â†’ Factor TermTail
Factor         â†’ ( Expr ) | ID FactorTail | Literal
```

*See `Grammar.txt` for complete grammar specification.*

---

## ğŸ›  Implementation Details

### Parser Architecture

- **Class**: `Parser` in `parser.py`
- **Method**: Recursive-descent with predictive parsing
- **Token Management**: 
  - `peek(n)` - Look ahead n tokens (skipping trivia)
  - `advance()` - Consume current token
  - `expect(type, lexeme)` - Validate and consume expected token
- **Error Handling**: Raises `ParserError` with descriptive messages

### Key Parser Methods

| Method | Grammar Rule | Description |
|--------|--------------|-------------|
| `parse_program()` | Program â†’ DeclList EOF | Entry point |
| `parse_decl()` | Decl â†’ VarDecl \| FunDecl | Declaration with lookahead |
| `parse_compound_stmt()` | CompoundStmt â†’ { ... } | Scoped block |
| `parse_expr()` | Expr â†’ AssignExpr | Expression entry |
| `parse_assign_expr()` | AssignExpr â†’ ID = ... | Assignment with lookahead |
| `parse_factor()` | Factor â†’ ... | Primary expressions |

### Lookahead Strategy

The parser uses lookahead to disambiguate:
- **Variable vs Function**: After `TypeSpec ID`, checks for `(` to distinguish
- **Assignment vs Expression**: After `ID`, checks for `=` operator
- **Statement boundaries**: Identifies statement types by first keyword/symbol

---

## ğŸ§ª Testing

### Test File: `SourceCode.c`
The project includes a sample C source file with:
- Variable declarations (single and multiple)
- Function definitions
- Control structures (`if`/`else`)
- Expressions with operators
- Comments (single-line and multi-line)
- Return statements

### Running Tests
```bash
cd src
python main.py SourceCode.c -o result.json
```

Expected output: `Syntax: OK` (indicates successful parsing)

---

## ğŸ¯ Current Status

### âœ… Completed Features

**Scanner (Lexical Analyzer):**
- âœ… Complete tokenization of C-like syntax
- âœ… All token types recognized (keywords, identifiers, operators, literals, special chars)
- âœ… Single-line and multi-line comment handling
- âœ… Token statistics and reporting
- âœ… JSON output generation

**Parser (Syntax Analyzer):**
- âœ… Full recursive-descent parser implementation
- âœ… Complete grammar coverage for C-like language
- âœ… Variable declarations (single and multiple)
- âœ… Function declarations with parameters
- âœ… Compound statements with local scopes
- âœ… Control flow (`if`/`else`, `while`, `for`, `return`)
- âœ… Expression parsing with proper precedence
- âœ… Assignment expressions
- âœ… Logical operators (`||`, `&&`)
- âœ… Relational operators (`<`, `<=`, `>`, `>=`, `==`, `!=`)
- âœ… Arithmetic operators (`+`, `-`, `*`, `/`)
- âœ… Function calls with arguments
- âœ… Parenthesized expressions
- âœ… Trivia handling (auto-skip whitespace/comments)
- âœ… Multi-token lookahead support
- âœ… Comprehensive error reporting

---

## ğŸ› Known Issues

- Parser does not currently track line numbers for error reporting
- No recovery mechanism - parsing stops at first syntax error
- Limited test coverage for edge cases
- Character constants recognized but not fully validated

---

## ğŸš§ Future Enhancements

1. **Full C Compliance**: Extend grammar to support complete C syntax
2. **Optimization Passes**: Implement compiler optimization techniques
3. **Target Code Generation**: Generate executable code for specific architectures
4. **Interactive Mode**: REPL for testing expressions and statements
5. **Debug Mode**: Verbose output showing parser state transitions
6. **Performance**: Optimize tokenization and parsing for large files

---

## ğŸ“š References

- **Compiler Design Theory**: Aho, Sethi, Ullman - "Compilers: Principles, Techniques, and Tools"
- **Recursive-Descent Parsing**: Top-down predictive parsing technique
- **C Language Specification**: ANSI C / ISO C standards
- **Python Implementation**: Modern Python 3.7+ features (type hints, f-strings)

---

## ğŸ‘¥ Project Information

**Course**: Compiler Design Lab  
**Institution**: College  
**Branch**: main  
**Last Updated**: December 11, 2025

---

## ğŸ“„ License

This project is developed for educational purposes as part of a Compiler Design course.
